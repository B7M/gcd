In the introductory lesson, we mentioned that there is a lot of data available for analysis on the Internet, which probably comes at no surprise given the vast size of the Internet. Sometimes these data are in a nice CSV format that we can quickly pull from the Internet. Sometimes, the data are spread across a web page, and it's our job to "scrape" that information from the webpage and get it into a usable format. Knowing first that this is possible within R and second, having some idea of where to start is an important beginning step to get data from the Internet. Picture an example of wanting to start a company but not knowing exactly what people you'll need. So, you go to the websites of a bunch of companies similar to the company you want to start and find all the names and titles of the people working there. You then compare the titles across companies and voila, you've got a better idea of who you'll need at your new company. You could imagine that while this information may be helpful to have, getting it manually would be a pain. Navigating to each site individually, finding the information, copying and pasting each name. That sounds awful! Thankfully, there's a way to scrape the web from R directly!

This uses the helpful package R Vest. It gets its name from the word "harvest." The idea here is you'll use this package to "harvest" information from websites! However, as you may imagine, this is less straightforward than pulling data that are already formatted the way you want them (as we did with APIs), since we'll have to do some extra work to get everything in order. First, let's install the R Vest package, if you haven't already done so, using the install dot packages function. When R Vest is given a webpage (URL) as input, an R Vest function reads in the HTML code from the webpage. HTML is the language websites use to display everything you see on the website. You've seen HTML documents before, as this is one of the formats that you can Knit to from an R Markdown (dot RMD) document! Generally, all HTML documents require each webpage to have a similar structure. This structure is specified by using different tags. For example, a header at the top of your webpage would use a specific tag. Website links would use a different tag. These different tags help to specify how the website should appear. R Vest takes advantage of these tags to help you extract the parts of the webpage you're most interested in. So let's see exactly how to do all of this with an example.

But first, to use R Vest, there is a tool that will make your life a lot easier. It's called Selector Gadget. It's a "javascript bookmarklet." What this means for us is that we'll be able to go to a webpage, turn on Selector Gadget, and help figure out how to appropriately specify what components from the webpage we want to extract using R Vest. To get started using Selector Gadget, you'll have to enable the Chrome Extension. To enable Selector Gadget Click the provided link to open up the Selector Gadget Chrome Extension and Click "Add to Chrome". 

In the pop-up, Click "Add extension"

Selector Gadget's icon will now be visible to the right of the web address bar within Google Chrome. You will click on this to use Selector Gadget.

Similar to the example above, what if you were interested in knowing the most popular books on the Project Gutenburg library and how many times they had been downloaded in the last week? Sure, you could go to their website and copy and paste each title and download number into a spreadsheet. But, that's not very fun! Alternatively, you could write and run a few lines of code and get all the information that way! We'll do that in the following example. To use Selector Gadget, navigate to the webpage you're interested in scraping, in this case, http://www.gutenberg.org/ebooks/search/?sort_order=downloads, 

and toggle Selector Gadget by clicking on the Selector Gadget icon. A menu at the bottom-right of your web page should appear.

Now that Selector Gadget has been toggled, as you mouse over the page, colored boxes should appear. We'll click on the the title of one of the books to tell Selector Gadget that we are interested in that component of the page. (Click on the question mark symbol in the little Selector Gadget menu for a quick primer on how it works!)

In doing so, we can see that other components of the webpage that Selector Gadget has deemed similar to what you've clicked will be highlighted. And, text will show up in the menu at the bottom of the page letting you know what you should use in R Vest to specify the part of the webpage you're most interested in extracting. Here, we see with that Selector Gadget has highlighted the book titles! Perfect. But we also said we wanted to know how many times each book was downloaded. 

So mouse over that section of the page for one of the books and click on it as well. 

Uh oh. A bunch of different parts of the website are now deemed similar and have been highlighted yellow - we don't want that! Those other things in yellow should not be included in our R Vest search later! Let's fix this. 

First, mouse over the author names of the books that have been mistakenly included and you should see a red outline appear. 

If you click while that red outline is present, it should remove all the items that are deemed similar to that item and exclude elements like that from your Selector Gadget search. 

Okay, we're getting closer! Just one more thing - the "Sort Alphabetically" and "Sort by Release Date" are still highlighted in yellow. Mouse over one of those elements until it is outlined in red and click. Now the only things on the webpage that are yellow (and thus included in your Selector Gadget terms) are the book titles and the number of times they've been downloaded. With this done, Selector Gadget has found the correct element tags that we will give to R Vest to extract just the information we want! 

Now we're ready to use R Vest's functions. We'll use the function read HTML to read in the HTML from our webpage of interest. We'll then use HTML nodes to specify which parts of the webpage we want to extract. Within this function we specify " dot extra ,  dot booklink dot title", as that's what Selector Gadget told us to specify to "harvest" the information we're interested in. Finally HTML underscore text extracts the text from the tag we've specified, giving us that list of books and their download numbers that we wanted to see! With just a few lines of code we have the information we were looking for!

From here, now that the data is in R, we can format it however we like and manipulate the data! Briefly, let's turn it into a dataframe and make a simple plot comparing the number of downloads! First, Save the results to a vector and then rearrange the data so that the titles are in one column and the downloads are in another. We then Turn this into a dataframe for plotting and rename the columns. Finally in wrangling our data, we Remove the word "downloads" from the numbers and convert them to numeric. Now that our data is in the correct form, we can Plot the number of times a book has been downloaded using g g plot 2. We specify which data to plot, indicate that we want a barplot, and flip the axes so that the book titles are on the Y axis and thus are horizontal. And there you have it! You've scraped the information from the Project Gutenberg website and produced a graph of the data! You can tweak your g g plot  command to modify nearly all aspects of the plots - including the order of the books, the colours of the bars, the background and axes colours - the possibilities are endless! Play with this!

In this lesson we learned how to use R Vest and Selector Gadget to scrape information from the web. R Vest is an incredibly useful R package to load HTML data into R, and Selector Gadget is a very useful extension that allows you to narrow down the parameters you provide to R Vest! We then worked through an example using both of these tools to scrape information on ebook downloads from the Project Gutenberg, which we then plotted within R.. Between this lesson and the last, we hope you are beginning to feel comfortable gathering data from the internet - either formally through an API or by web scraping. 